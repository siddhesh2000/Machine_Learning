# -*- coding: utf-8 -*-
"""House prices prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j6FLte0K43obK8Yv06mlIwOWcCFSgvra

# Part 1: Data Preprocessing

Dataset: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data?select=train.csv

## Importing the libraries and dataset
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

dataset = pd.read_csv('/content/train.csv')

"""## Data exploration"""

dataset.head()

dataset.shape

dataset.columns

dataset.info()

# statistical summary
dataset.describe()

# Numerical columns
dataset.select_dtypes(include=['int64', 'float64']).columns

len(dataset.select_dtypes(include=['int64', 'float64']).columns)

# categorical columns
dataset.select_dtypes(include='object').columns

len(dataset.select_dtypes(include='object').columns)

"""## Dealing with null values"""

dataset.isnull().values.any()

dataset.isnull().values.sum()

dataset.isnull().sum()

# columns with null values
dataset.columns[dataset.isnull().any()]

len(dataset.columns[dataset.isnull().any()])

# null values with heatmap
plt.figure(figsize=(16,9))
sns.heatmap(dataset.isnull())
plt.show()

dataset.shape

null_percent = dataset.isnull().sum() / dataset.shape[0] * 100

# (missing values / total values) * 100

null_percent

# columns to drop (more than 50% null values)
cols_to_drop = null_percent[null_percent > 50].keys()

cols_to_drop

dataset = dataset.drop(columns=['Alley', 'PoolQC', 'Fence', 'MiscFeature'])

dataset.shape

# columns with null values
dataset.columns[dataset.isnull().any()]

len(dataset.columns[dataset.isnull().any()])

"""**Add columns mean to numerical columns**"""

# Numerical Columns
# 'LotFrontage', 'MasVnrArea', 'GarageYrBlt'

dataset['LotFrontage'] = dataset['LotFrontage'].fillna(dataset['LotFrontage'].mean())
dataset['MasVnrArea'] = dataset['MasVnrArea'].fillna(dataset['MasVnrArea'].mean())
dataset['GarageYrBlt'] = dataset['GarageYrBlt'].fillna(dataset['GarageYrBlt'].mean())

len(dataset.columns[dataset.isnull().any()])

"""**Add columns mode to categorical columns**"""

dataset.select_dtypes(include='object').columns

dataset.columns[dataset.isnull().any()]

len(dataset.columns[dataset.isnull().any()])

dataset['MasVnrType'] = dataset['MasVnrType'].fillna(dataset['MasVnrType'].mode()[0])
dataset['BsmtQual'] = dataset['BsmtQual'].fillna(dataset['BsmtQual'].mode()[0])
dataset['BsmtCond'] = dataset['BsmtCond'].fillna(dataset['BsmtCond'].mode()[0])
dataset['BsmtExposure'] = dataset['BsmtExposure'].fillna(dataset['BsmtExposure'].mode()[0])
dataset['BsmtFinType1'] = dataset['BsmtFinType1'].fillna(dataset['BsmtFinType1'].mode()[0])
dataset['BsmtFinType2'] = dataset['BsmtFinType2'].fillna(dataset['BsmtFinType2'].mode()[0])
dataset['Electrical'] = dataset['Electrical'].fillna(dataset['Electrical'].mode()[0])
dataset['FireplaceQu'] = dataset['FireplaceQu'].fillna(dataset['FireplaceQu'].mode()[0])
dataset['GarageType'] = dataset['GarageType'].fillna(dataset['GarageType'].mode()[0])
dataset['GarageFinish'] = dataset['GarageFinish'].fillna(dataset['GarageFinish'].mode()[0])
dataset['GarageQual'] = dataset['GarageQual'].fillna(dataset['GarageQual'].mode()[0])
dataset['GarageCond'] = dataset['GarageCond'].fillna(dataset['GarageCond'].mode()[0])

len(dataset.columns[dataset.isnull().any()])

dataset.isnull().values.any()

"""## Distplot"""

# distplot of the target variable

plt.figure(figsize=(16,9))
bar = sns.distplot(dataset['SalePrice'])
bar.legend(["Skewness: {:.2f}".format(dataset['SalePrice'].skew())])
plt.show()

"""## Correlation matrix"""

dataset_2 = dataset.drop(columns='SalePrice')

dataset_2.shape

dataset_2.corrwith(dataset['SalePrice']).plot.bar(
    figsize=(16,9), title='Correlated with SalePrice', grid=True
)

# heatmap
plt.figure(figsize=(25, 25))
ax = sns.heatmap(data=dataset.corr(), cmap='coolwarm', annot=True, linewidths=2)

high_corr = dataset.corr()

high_corr_features = high_corr.index[abs(high_corr['SalePrice']) > 0.5]

high_corr_features

len(high_corr_features)

# heatmap
plt.figure(figsize=(16,9))
ax = sns.heatmap(data=dataset[high_corr_features].corr(), cmap='coolwarm', annot=True, linewidths=2)

"""## Dealing with the categorical values"""

dataset.shape

# categorical columns
dataset.select_dtypes(include='object').columns

len(dataset.select_dtypes(include='object').columns)

dataset = pd.get_dummies(data=dataset, drop_first=True)

dataset.shape

# categorical columns
dataset.select_dtypes(include='object').columns

len(dataset.select_dtypes(include='object').columns)

"""## Splitting the dataset"""

# independ variables / matrix of features
x = dataset.drop(columns='SalePrice')

# target variable / dependent variable
y = dataset['SalePrice']

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)

x_train.shape

y_train.shape

x_test.shape

y_test.shape

"""## Feature scaling"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

x_train

x_test

"""# Part 2: Building the model

## 1) Multiple linear regression
"""

from sklearn.linear_model import LinearRegression
regressor_mlr = LinearRegression()
regressor_mlr.fit(x_train, y_train)

y_pred = regressor_mlr.predict(x_test)

from sklearn.metrics import r2_score

r2_score(y_test, y_pred)

"""## 2) Random forest regression"""

from sklearn.ensemble import RandomForestRegressor
regressor_rf = RandomForestRegressor()
regressor_rf.fit(x_train, y_train)

y_pred = regressor_rf.predict(x_test)

from sklearn.metrics import r2_score
r2_score(y_test, y_pred)

"""## 3) XGBoost regression"""

from xgboost import XGBRFRegressor
regressor_xgb = XGBRFRegressor()
regressor_xgb.fit(x_train, y_train)

y_pred = regressor_xgb.predict(x_test)

from sklearn.metrics import r2_score
r2_score(y_test, y_pred)

"""# Part 3: Hyper parameter tuning"""

from sklearn.model_selection import RandomizedSearchCV

parameters = {
    'n_estimators':[200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000],
    'max_depth':[10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],
    'min_samples_split':[2, 5, 10],
    'min_samples_leaf':[1, 2, 4],
    'max_features':['auto', 'sqrt'],
    'bootstrap':[True, False]    
}

parameters

random_cv = RandomizedSearchCV(estimator=regressor_rf, param_distributions=parameters, n_iter=50, cv=5,
                               verbose=2, n_jobs=-1, random_state=0)

random_cv.fit(x_train, y_train)

random_cv.best_estimator_

random_cv.best_params_

"""# Part 4: Final model (Random forest regressor)"""

from sklearn.ensemble import RandomForestRegressor
regressor = RandomForestRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',
                      max_depth=50, max_features='sqrt', max_leaf_nodes=None,
                      max_samples=None, min_impurity_decrease=0.0,
                      min_impurity_split=None, min_samples_leaf=1,
                      min_samples_split=2, min_weight_fraction_leaf=0.0,
                      n_estimators=1200, n_jobs=None, oob_score=False,
                      random_state=None, verbose=0, warm_start=False)
regressor.fit(x_train, y_train)

y_pred = regressor.predict(x_test)

from sklearn.metrics import r2_score
r2_score(y_test, y_pred)